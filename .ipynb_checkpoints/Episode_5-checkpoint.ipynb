{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section>\n",
    "    <div><img src=\"./images/Zahra-Amini.jpg\"></div>\n",
    "    <div style=\"display: flex\">\n",
    "        <div style=\"width:50%;background:#62060b\">\n",
    "            <div style=\"margin:auto;width:15rem\">\n",
    "                <img src=\"./images/filoger.png\" style=\"width:9rem; padding: 0.5rem 0;display:inline-block; vertical-align: middle\">\n",
    "                <p style=\"display:inline-block;font-family:monospace;font-weight:bold;font-size:15pt;color:white\">\n",
    "                Filoger\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "        <div style=\"width:50%;background:#606368\">\n",
    "            <div style=\"margin:auto;width:23rem;margin-top: 3rem;\">\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:#62060b;text-align:center\">\n",
    "                Deep Learning Course\n",
    "                <p>\n",
    "                <p style=\"font-family:monospace;font-weight:bold;font-size:15pt;color:white;text-align:center\">\n",
    "                Episode 5\n",
    "                <p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </div>\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7dn3JN5sbpxS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# تنظیم seed برای numpy و tenserflow\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat-GPT Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <h4>مفهوم Seed:</h4>\n",
    "    وقتی توی برنامه‌نویسی از اعداد تصادفی استفاده می‌کنیم (مثل انتخاب اعداد تصادفی یا مقداردهی اولیه وزن‌ها در شبکه‌های عصبی)، این اعداد در واقع به صورت کاملاً تصادفی نیستند، بلکه از یه الگوریتم مشخصی تولید می‌شن. این الگوریتم‌ها نیاز به یه مقدار اولیه (که بهش میگیم Seed) دارن تا تولید اعداد تصادفی رو شروع کنن. اگه Seed یکسان باشه، خروجی‌های تصادفی هم یکسان خواهند بود.\n",
    "    <h4>دلیل استفاده از Seed:</h4>\n",
    "    <b>قابل تکرار بودن:</b> وقتی Seed رو ثابت تنظیم می‌کنی، هر بار که برنامه رو اجرا می‌کنی، همون خروجی رو می‌گیری. این برای تست و دیباگ کردن برنامه‌ها خیلی مفیده، چون می‌دونی دقیقاً چه خروجی‌ای باید بگیری.<br/>\n",
    "    <b>کنترل بیشتر:</b> اگه بخوای نتایج مختلف رو با هم مقایسه کنی و فقط یک متغیر رو تغییر بدی، این کار کمک می‌کنه تا اثرات اون متغیر رو راحت‌تر بسنجی.\n",
    "    <h4>بخش‌های کد:</h4>\n",
    "</section>\n",
    "\n",
    "<code style=\"display:block;\">np.random.seed(42)</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    این خط Seed رو برای توابع تصادفی NumPy تنظیم می‌کنه. بعد از این، هر عدد تصادفی که با NumPy تولید می‌شه (مثل np.random.rand() یا np.random.randint()) بر اساس Seed 42 خواهد بود.\n",
    "</section>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">tf.random.set_seed(42)</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    این خط Seed رو برای توابع تصادفی TensorFlow تنظیم می‌کنه. توابع تصادفی در TensorFlow (مثل مقداردهی اولیه وزن‌ها در لایه‌های شبکه عصبی) از این Seed برای تولید اعداد تصادفی استفاده می‌کنن.\n",
    "    <h4>نتیجه:</h4>\n",
    "    با تنظیم Seed یکسان برای NumPy و TensorFlow، می‌تونی مطمئن باشی که خروجی‌های تصادفی این دو کتابخانه در دفعات مختلف اجرای کد ثابت خواهند بود، که این موضوع برای تست و تحقیق بسیار مهمه.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Scu3nKsIb0KK"
   },
   "outputs": [],
   "source": [
    "data = load_breast_cancer()\n",
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    دیتای سرطان پستان رو میاریم.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ur-XgWxvb-nB"
   },
   "outputs": [],
   "source": [
    "X = data.data\n",
    "\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "nIBxJutzcJPH",
    "outputId": "92df1416-523a-4d49-cd7d-a298d1eae08e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BaFnj2xecWU7",
    "outputId": "31668ced-2699-411f-d5da-c13006820674"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "oBt2ipWWc_z0"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "lf54IZtWcdIg"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teacher Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    زمانی که StandardScaler را فراخوانی میکنیم، مدل scaler ساخته می شود.<br/>\n",
    "    یادمون باشه که قبل از اینکه اسکیل کنیم داده هارو، بایستی X ها رو به دو قسمت test و train تقسیم بندی کنیم.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F71UWA1odlLL",
    "outputId": "76f04269-f29a-4a97-9987-512deca89205"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E3CFxvfPdplS",
    "outputId": "47a78e41-7fa1-4e58-ba3e-3ce3f6d9816f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <h4>پیش‌زمینه:</h4>\n",
    "    در یادگیری ماشین، معمولاً داده‌ها به دو بخش اصلی تقسیم می‌شن:<br/>\n",
    "    <b>ویژگی‌ها (Features):</b> داده‌هایی که مدل ازشون برای یادگیری استفاده می‌کنه. معمولاً بهشون می‌گن X.<br/>\n",
    "    <b>برچسب‌ها (Labels):</b> داده‌هایی که مدل سعی می‌کنه پیش‌بینی کنه. معمولاً بهشون می‌گن y.<br/>\n",
    "    کدی که نوشتی روی y_train و y_test که همون برچسب‌ها هستن، کار می‌کنه.\n",
    "    <h4>تحلیل کد:</h4>\n",
    "</section>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    <b>reshape(-1, 1):</b>\n",
    "    این تابع شکل (شکل هندسی) آرایه رو تغییر می‌ده.<br/>\n",
    "    <b>-1</b> به این معناست که پایتون به صورت خودکار تعداد نمونه‌ها (ابعاد) رو محاسبه می‌کنه. مثلا، اگه y_train شامل 100 نمونه باشه، -1 تبدیل به 100 می‌شه.<br/>\n",
    "    <b>1</b> به این معناست که هر نمونه به صورت یک ستون (با یک ویژگی یا مقدار) درمیاد. بنابراین، آرایه نهایی به شکل (تعداد نمونه‌ها, 1) درمیاد.<br/>\n",
    "    به عبارتی، اگه y_train و y_test قبلاً به شکل (تعداد نمونه‌ها,) بودن (یعنی یک آرایه تک‌بعدی)، بعد از این تغییر به شکل (تعداد نمونه‌ها, 1) درمیاد (یعنی یک آرایه دوبعدی که هر نمونه به صورت یک ستون مجزا نگهداری می‌شه).\n",
    "    <h4>دلیل استفاده:</h4>\n",
    "    در بعضی از مدل‌های یادگیری ماشین، مثل رگرسیون خطی یا شبکه‌های عصبی، برچسب‌ها باید به صورت یک آرایه دوبعدی با شکل (n_samples, 1) (جایی که n_samples تعداد نمونه‌هاست) به مدل داده بشن. این تغییر شکل اطمینان می‌ده که داده‌ها به فرم درستی برای تغذیه به مدل آماده هستن.\n",
    "    <h4>نتیجه:</h4>\n",
    "    حالا با استفاده از این کد، برچسب‌های y_train و y_test به شکل صحیحی درمیاد که برای پردازش و آموزش مدل بهینه‌تره.\n",
    "    <h4>و در نهایت:</h4>\n",
    "</section>\n",
    "\n",
    "<code style=\"display:block; text-align:left;direction:ltr\">y.shape</code>\n",
    "\n",
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    این خط از کد، شکل آرایه y رو بهت نشون می‌ده، یعنی تعداد نمونه‌ها و تعداد ویژگی‌ها (ستون‌ها) در اون آرایه. این برای چک کردن اینه که مطمئن بشی تغییر شکلی که انجام دادی درست انجام شده.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QNBXqdKOdpt5"
   },
   "outputs": [],
   "source": [
    "# sigmoid, RelU, leaky RelU\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "def build_model(activation, activation_name):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(30, input_dim = 30, activation = activation, kernel_initializer = initializer))\n",
    "  for _ in range(8):\n",
    "    model.add(tf.keras.layers.Dense(30, activation = activation, kernel_initializer = initializer))\n",
    "  model.add(tf.keras.layers.Dense(1, activation = 'sigmoid', kernel_initializer = initializer))\n",
    "\n",
    "  model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "  history = model.fit(X_train, y_train, epochs=50, verbose=0, validation_data=(X_test, y_test))\n",
    "\n",
    "  loss_after, accuracy_after = model.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "  with tf.GradientTape() as tape:\n",
    "    predictions = model(X_train)\n",
    "    loss = tf.keras.losses.binary_crossentropy(y_train, predictions)\n",
    "  grads = tape.gradient(loss, model.trainable_weights)\n",
    "  grad_norms = [np.linalg.norm(grad) for grad in grads if grad is not None]\n",
    "\n",
    "  print(f\"Loss with {activation_name} after training: {loss_after:.6f}\")\n",
    "  print(f\"Accuracy with {activation_name} after training: {accuracy_after:.6f}\")\n",
    "  for i, norm in enumerate(grad_norms, 1):\n",
    "      print(f\"Gradient Norm of Layer {i}: {norm:.6f}\")\n",
    "\n",
    "  return history, grad_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<section style=\"text-align:right;direction:rtl;\">\n",
    "    این کدی که فرستادی، به‌طور کامل فرآیند ساخت، آموزش، و ارزیابی یک مدل شبکه عصبی رو توضیح می‌ده و به ویژه به تأثیر نوع تابع فعال‌سازی روی عملکرد مدل و مشکل \"نابودی گرادیان\" (Vanishing Gradient) می‌پردازه. بیایید خط به خط این کد رو تحلیل کنیم.\n",
    "    <h4>1. مقداردهی اولیه (Initializer)</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        initializer = tf.keras.initializers.GlorotUniform(seed=42)\n",
    "    </code>\n",
    "    <b>Glorot Uniform:</b> این نوع مقداردهی اولیه برای وزن‌ها (weights) طراحی شده تا مدل سریع‌تر و مؤثرتر یاد بگیره. این روش به‌ویژه برای لایه‌هایی که از توابع فعال‌سازی مثل sigmoid و tanh استفاده می‌کنند، مؤثره.<br/>\n",
    "    <b>Seed:</b> همون‌طور که قبلاً توضیح دادیم، seed=42 باعث می‌شه تا خروجی‌های تصادفی این مقداردهی اولیه ثابت باشه و هر بار که مدل رو اجرا می‌کنیم، همون وزن‌های اولیه تولید بشه.\n",
    "    <h4>2. ساخت مدل (build_model function)</h4>\n",
    "    <section style=\"text-align:right;direction:rtl;\">\n",
    "        def build_model(activation, activation_name):\n",
    "    </section>\n",
    "    این تابع، یک مدل شبکه عصبی می‌سازه که از تابع فعال‌سازی مشخصی (که به عنوان پارامتر activation بهش داده می‌شه) استفاده می‌کنه.\n",
    "activation_name برای نمایش نام تابع فعال‌سازی در خروجی استفاده می‌شه.\n",
    "    <h4>3. افزودن لایه‌های مدل</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(tf.keras.layers.Dense(30, input_dim = 30, activation = activation, kernel_initializer = initializer))\n",
    "    </code>\n",
    "    <b>Sequential:</b> نوعی از مدل که لایه‌ها رو به ترتیب پشت سر هم می‌چینه.<br/>\n",
    "    <b>Dense Layer:</b> لایه کاملاً متصل (Fully Connected) با 30 نورون. input_dim=30 یعنی ورودی لایه 30 ویژگی داره.<br/>\n",
    "    <b>activation:</b> اینجا از تابع فعال‌سازی‌ای که به تابع build_model داده شده، استفاده می‌شه.<br/>\n",
    "    <b>kernel_initializer:</b> از مقداردهی اولیه‌ای که تعریف کردیم (Glorot Uniform) برای وزن‌ها استفاده می‌شه.\n",
    "    <h4>4. افزودن لایه‌های پنهان (Hidden Layers)</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        for _ in range(8):\n",
    "            model.add(tf.keras.layers.Dense(30, activation = activation, kernel_initializer = initializer))\n",
    "    </code>\n",
    "    این کد یک حلقه for برای اضافه کردن 8 لایه پنهان با 30 نورون در هر کدوم اجرا می‌کنه. این لایه‌ها هم از همون تابع فعال‌سازی و مقداردهی اولیه استفاده می‌کنن.\n",
    "    <h4>5. لایه خروجی</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        model.add(tf.keras.layers.Dense(1, activation = 'sigmoid', kernel_initializer = initializer))\n",
    "    </code>\n",
    "    لایه خروجی فقط یک نورون داره و تابع فعال‌سازی sigmoid رو استفاده می‌کنه، که مناسب برای مسائل دسته‌بندی باینریه.\n",
    "    <h4>6. کامپایل مدل</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.1), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    </code>\n",
    "    <b>Optimizer:</b> از SGD (Stochastic Gradient Descent) با نرخ یادگیری 0.1 استفاده می‌کنیم. SGD یک الگوریتم ساده و پایه‌ای برای بهینه‌سازی مدل‌هاست.<br/>\n",
    "    <b>Loss Function:</b> تابع زیان binary_crossentropy برای مسائل دسته‌بندی باینری مناسبه.<br/>\n",
    "    <b>Metrics:</b> دقت (accuracy) به عنوان معیار عملکرد انتخاب شده.\n",
    "    <h4>7. آموزش مدل</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        history = model.fit(X_train, y_train, epochs=50, verbose=0, validation_data=(X_test, y_test))\n",
    "    </code>\n",
    "    <b>fit:</b> مدل رو برای 50 دوره (epoch) با داده‌های آموزشی X_train و y_train آموزش می‌ده و همزمان عملکردش رو روی داده‌های تست X_test و y_test ارزیابی می‌کنه.\n",
    "    <h4>8. ارزیابی مدل</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        loss_after, accuracy_after = model.evaluate(X_test, y_test, verbose=0)\n",
    "    </code>\n",
    "    بعد از آموزش، مدل روی داده‌های تست ارزیابی می‌شه و میزان زیان و دقت خروجی داده می‌شه.\n",
    "    <h4>9. محاسبه گرادیان‌ها</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(X_train)\n",
    "            loss = tf.keras.losses.binary_crossentropy(y_train, predictions)\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\n",
    "        grad_norms = [np.linalg.norm(grad) for grad in grads if grad is not None]\n",
    "    </code>\n",
    "    <b>GradientTape:</b> ابزاری در TensorFlow که برای محاسبه گرادیان‌ها استفاده می‌شه.<br/>\n",
    "    اینجا، از این ابزار استفاده می‌کنیم تا گرادیان زیان نسبت به وزن‌های مدل رو حساب کنیم.<br/>\n",
    "    سپس grad_norms که شامل نُرم (یا اندازه) گرادیان‌های هر لایه است رو محاسبه می‌کنیم.\n",
    "    <h4>10. نمایش نتایج</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        print(f\"Loss with {activation_name} after training: {loss_after:.6f}\")\n",
    "        print(f\"Accuracy with {activation_name} after training: {accuracy_after:.6f}\")\n",
    "        for i, norm in enumerate(grad_norms, 1):\n",
    "            print(f\"Gradient Norm of Layer {i}: {norm:.6f}\")\n",
    "    </code>\n",
    "    <b>Loss and Accuracy:</b> زیان و دقت نهایی مدل رو بعد از آموزش نمایش می‌ده.<br/>\n",
    "    <b>Gradient Norms:</b> اندازه‌ی گرادیان‌های هر لایه رو نمایش می‌ده، که کمک می‌کنه ببینیم آیا مشکل نابودی گرادیان وجود داره یا نه.\n",
    "    <h4>11. خروجی نهایی:</h4>\n",
    "    <code style=\"display:block; text-align:left;direction:ltr\">\n",
    "        return history, grad_norms\n",
    "    </code>\n",
    "    در نهایت، تاریخچه آموزش (که شامل زیان و دقت در هر دوره است) و نُرم گرادیان‌ها رو برمی‌گردونه.\n",
    "    <h4>تأثیر تابع فعال‌سازی:</h4>\n",
    "    با اجرای این تابع برای توابع فعال‌سازی مختلف (مثل sigmoid, ReLU, leaky ReLU) می‌تونی تأثیر نوع تابع فعال‌سازی رو روی عملکرد مدل و گرادیان‌ها بررسی کنی. sigmoid به‌خصوص در لایه‌های پنهان می‌تونه باعث نابودی گرادیان بشه، چون گرادیان‌های خروجی این تابع وقتی ورودی خیلی بزرگ یا خیلی کوچیک می‌شه، نزدیک صفر می‌شن. اما ReLU و leaky ReLU از این مشکل جلوگیری می‌کنن چون شیب اون‌ها در بیشتر ناحیه‌ها صفر نیست.\n",
    "</section>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9_Lfc8UHdp8_",
    "outputId": "65de1cc4-408f-4900-c63a-2f10ac7e46d7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MA_Malekzadeh\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss with sigmoid after training: 0.671425\n",
      "Accuracy with sigmoid after training: 0.622807\n",
      "Gradient Norm of Layer 1: 0.000844\n",
      "Gradient Norm of Layer 2: 0.000026\n",
      "Gradient Norm of Layer 3: 0.001350\n",
      "Gradient Norm of Layer 4: 0.000209\n",
      "Gradient Norm of Layer 5: 0.003051\n",
      "Gradient Norm of Layer 6: 0.001063\n",
      "Gradient Norm of Layer 7: 0.015964\n",
      "Gradient Norm of Layer 8: 0.005523\n",
      "Gradient Norm of Layer 9: 0.077086\n",
      "Gradient Norm of Layer 10: 0.027043\n",
      "Gradient Norm of Layer 11: 0.396481\n",
      "Gradient Norm of Layer 12: 0.139465\n",
      "Gradient Norm of Layer 13: 1.750329\n",
      "Gradient Norm of Layer 14: 0.615336\n",
      "Gradient Norm of Layer 15: 8.014051\n",
      "Gradient Norm of Layer 16: 2.816982\n",
      "Gradient Norm of Layer 17: 21.476465\n",
      "Gradient Norm of Layer 18: 7.566497\n",
      "Gradient Norm of Layer 19: 70.101166\n",
      "Gradient Norm of Layer 20: 25.781906\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not interpret activation function identifier: ReLU",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m grad_norms \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      4\u001b[0m histories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m], grad_norms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mbuild_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m histories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m], grad_norms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReLU\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReLU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReLU\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m histories[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeaky ReLU\u001b[39m\u001b[38;5;124m'\u001b[39m], grad_norms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeaky ReLU\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m build_model(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mLeakyReLU(alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLeaky ReLU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[17], line 5\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(activation, activation_name)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbuild_model\u001b[39m(activation, activation_name):\n\u001b[0;32m      4\u001b[0m   model \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mSequential()\n\u001b[1;32m----> 5\u001b[0m   model\u001b[38;5;241m.\u001b[39madd(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_dim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):\n\u001b[0;32m      7\u001b[0m     model\u001b[38;5;241m.\u001b[39madd(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m30\u001b[39m, activation \u001b[38;5;241m=\u001b[39m activation, kernel_initializer \u001b[38;5;241m=\u001b[39m initializer))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:89\u001b[0m, in \u001b[0;36mDense.__init__\u001b[1;34m(self, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, lora_rank, **kwargs)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(activity_regularizer\u001b[38;5;241m=\u001b[39mactivity_regularizer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munits \u001b[38;5;241m=\u001b[39m units\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation \u001b[38;5;241m=\u001b[39m \u001b[43mactivations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias \u001b[38;5;241m=\u001b[39m use_bias\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernel_initializer \u001b[38;5;241m=\u001b[39m initializers\u001b[38;5;241m.\u001b[39mget(kernel_initializer)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\activations\\__init__.py:104\u001b[0m, in \u001b[0;36mget\u001b[1;34m(identifier)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(obj):\n\u001b[0;32m    103\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\n\u001b[1;32m--> 104\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not interpret activation function identifier: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midentifier\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    106\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: Could not interpret activation function identifier: ReLU"
     ]
    }
   ],
   "source": [
    "histories = {}\n",
    "grad_norms = {}\n",
    "\n",
    "histories['sigmoid'], grad_norms['sigmoid']=build_model('sigmoid', 'sigmoid')\n",
    "histories['ReLU'], grad_norms['ReLU']=build_model('ReLU', 'ReLU')\n",
    "histories['Leaky ReLU'], grad_norms['Leaky ReLU'] = build_model(tf.keras.layers.LeakyReLU(alpha=0.01), 'Leaky ReLU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 957
    },
    "id": "etKfpQdgeDq3",
    "outputId": "e0b2e5f7-8a9e-41c1-c77a-39f8a420baa8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "for key in histories:\n",
    "    plt.plot(histories[key].history['val_accuracy'], label=f'{key}')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "for key in histories:\n",
    "    plt.plot(histories[key].history['val_loss'], label=f'{key}')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "for key in grad_norms:\n",
    "    plt.plot(grad_norms[key], label=f'{key}')\n",
    "plt.title('Gradient Norms by Layer')\n",
    "plt.ylabel('Gradient Norm')\n",
    "plt.xlabel('Layer')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rGYWk7A7eDx5"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
